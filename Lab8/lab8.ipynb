{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d15a09-e8c2-4306-a8b9-05fb0bb7a905",
   "metadata": {},
   "source": [
    "# Лабораторная работа №8. Кластеризация данных. Обучение без учителя\n",
    "\n",
    "Если обучающий набор представляет собой множество образов \n",
    "$$\n",
    "T = \\{x_i\\}_{i=1}^N,\n",
    "$$ \n",
    "для которых правильная принадлежность к классам неизвестна, то в этом случае задачу разделения образов $x_i$ на классы (количество которых также может быть известно) называют задачей кластеризации данных, а построение классификатора — обучением без учителя.\n",
    "\n",
    "Более формально задачу кластеризации можно сформулировать в общем виде следующим образом. Пусть в пространстве векторов признаков $\\mathcal{X}$ задана некоторая обучающая выборка \n",
    "$$\n",
    "T = \\{x_1, \\dots, x_N\\} \\subset \\mathcal{X}\n",
    "$$ \n",
    "и определена метрика (расстояние) $\\rho(x_a, x_b)$. Необходимо разделить множество векторов $ T $ на непересекающиеся подмножества $C_k$ (назовём их **кластерами**):\n",
    "$$\n",
    "T = \\bigcup_{k=1}^K C_k, \\quad \\bigcap_{k=1}^K C_k = \\varnothing,\n",
    "$$\n",
    "так, чтобы векторы, принадлежащие одному и тому же кластеру, находились друг к другу на близком расстоянии (в смысле заданной метрики $\\rho$. При этом количество кластеров $\\mathcal{X}$ может быть как заданным заранее, так и быть найденным в результате решения задачи кластеризации.\n",
    "\n",
    "В рамках лабораторной работы мы рассмотрим два наиболее распространённых подхода к решению задачи кластеризации:\n",
    "1. **метод иерархической кластеризации на основе связей**;\n",
    "2. **метод K-средних**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ffcd2-38b1-490c-b971-65476ce28295",
   "metadata": {},
   "source": [
    "Для применения первого подхода на основе выбранной метрики (расстояния, связанной) необходимо предварительно задать **функцию межкластерного расстояния**. Для множеств $A = \\{a_i \\}$ и $B = \\{b_i \\}$ находить расстояние между ними $D(A, B)$, определяется одним из трёх следующих способов:\n",
    "\n",
    "1. **Ближайшей связью (single linkage):**\n",
    "   $$\n",
    "   D(A, B) = \\min_{a \\in A; b \\in B} \\rho(a, b);\n",
    "   $$\n",
    "\n",
    "2. **Средней связью (average linkage):**\n",
    "   $$\n",
    "   D(A, B) = \\frac{1}{|A| |B|} \\sum_{a \\in A; b \\in B} \\rho(a, b);\n",
    "   $$\n",
    "\n",
    "3. **Дальней связью (complete linkage):**\n",
    "   $$\n",
    "   D(A, B) = \\max_{a \\in A; b \\in B} \\rho(a, b).\n",
    "   $$\n",
    "\n",
    "Метод иерархической кластеризации множества \n",
    "$$\n",
    "T = \\{x_i\\}_{i=1}^N\n",
    "$$\n",
    "описывается в виде следующего несложного алгоритма:\n",
    "\n",
    "### Шаг 1.\n",
    "Задать начальное разбиение \n",
    "$$\n",
    "\\Omega = \\{C_1, \\dots, C_N\\}\n",
    "$$\n",
    "набора $T = \\{x_i\\}_{i=1}^N$ на одновекторные кластеры \n",
    "$$\n",
    "C_i = \\{x_i\\}, \\quad i = 1, \\dots, N.\n",
    "$$\n",
    "\n",
    "### Шаг 2.\n",
    "Найти два ближайших кластера $C_k$ и $C_l$, таких что\n",
    "$$\n",
    "D(C_k, C_l) = \\min_{C_k, C_l \\in \\Omega, \\, C_k \\neq C_l} D(C_k, C_l).\n",
    "$$\n",
    "\n",
    "### Шаг 3.\n",
    "Заменить в разбиении $\\Omega$ два ближайших кластера на один новый кластер $C_k \\cup C_l$, уменьшив на единицу значение числа кластеров $|\\Omega|$.\n",
    "\n",
    "### Шаг 4.\n",
    "Если достигнуто условие остановки, то закончить работу, иначе перейти на шаг 2.\n",
    "\n",
    "Условием завершения работы алгоритма на шаге 4 может выступать либо достижение желаемого (заданного) количества кластеров $|\\Omega|$, либо превышение некоторого порога ближайшего межкластерного расстояния:\n",
    "$$\n",
    "D(C_k, C_l) > D_{\\max}.\n",
    "$$\n",
    "\n",
    "Порог часто выбирают, например, как\n",
    "$$\n",
    "D_{\\max} = \\alpha \\max_{x_i, x_j \\in T} \\rho(x_i, x_j), \\alpha \\in (0,1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e027813-d1bc-4c3f-8350-ee6f063a17b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ab184-f30d-4ebd-bdd1-51578bb38083",
   "metadata": {},
   "source": [
    "## Задание 0\n",
    "Прочтите набор данных из директории _dataset_lab8_.\n",
    "Номер варианта - `mod(<порядковый номер в группе>, 10) + 1` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda9011-2dc0-43f8-bc48-46b94ac5fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_to_load = 10\n",
    "input_filename = f\"dataset/dataset_variant_{variant_to_load}.csv\"\n",
    "loaded_df = pd.read_csv(input_filename)\n",
    "data = loaded_df[['X1', 'X2']].values\n",
    "labels = loaded_df['Label'].values\n",
    "\n",
    "print(f\"Данные загружены из файла {input_filename}:\")\n",
    "print(loaded_df.head())\n",
    "\n",
    "# Визуализация загруженных данных\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=15)\n",
    "plt.title(f\"Вариант {variant_to_load}\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a6233-577e-4034-90f5-455114ea1c3d",
   "metadata": {},
   "source": [
    "## Задание №1\n",
    "Реализуйте функцию оценки точности кластеризации, используя только мэппинг меток и accuracy_score из библиотеки sklearn.\n",
    "Вы можете реализовать свою версию этой функции. Например, когда производится полный перебор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658a298-e386-48f5-a205-59fa0862127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_and_evaluate_accuracy(true_labels, cluster_labels):\n",
    "    \"\"\"\n",
    "    Оценка качества кластеризации по метрике accuracy и создание маппинга меток.\n",
    "\n",
    "    :param true_labels: numpy.ndarray, истинные метки.\n",
    "    :param cluster_labels: numpy.ndarray, метки кластеров.\n",
    "    :return: tuple (float, dict), где:\n",
    "        - float: значение accuracy.\n",
    "        - dict: маппинг меток кластеров в истинные метки.\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(true_labels))\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "    # Инициализация матрицы соответствий\n",
    "    cost_matrix = np.zeros((n_classes, n_clusters), dtype=int)\n",
    "\n",
    "    # TODO: Заполните матрицу cost_matrix\n",
    "    # Для каждой пары (истинная метка, метка кластера) подсчитайте количество совпадений\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_clusters):\n",
    "            # Подсчитайте, сколько объектов с истинной меткой i получили метку кластера j\n",
    "            cost_matrix[i, j] = ...  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "    # TODO: Примените Венгерский алгоритм для оптимального сопоставления\n",
    "    # Используйте функцию `linear_sum_assignment` из `scipy.optimize`\n",
    "    row_ind, col_ind = ...  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "    mapping = {col_ind[i]: row_ind[i] for i in range(len(row_ind))}\n",
    "    mapped_labels = np.array([mapping[label] for label in cluster_labels])\n",
    "    \n",
    "    # Вычисление accuracy\n",
    "    accuracy = accuracy_score(true_labels, mapped_labels)\n",
    "\n",
    "    return accuracy, mapping\n",
    "\n",
    "true_labels = np.array([0, 0, 1, 1, 2, 2])\n",
    "predicted_labels = np.array([2, 2, 0, 0, 1, 1])\n",
    "accuracy, mapping = match_and_evaluate_accuracy(true_labels, predicted_labels)\n",
    "assert accuracy == 1.0, 'Неверная реализация'\n",
    "\n",
    "true_labels = np.array([0, 1, 1, 2, 2, 2])\n",
    "predicted_labels = np.array([1, 0, 0, 2, 2, 2])\n",
    "accuracy, mapping = match_and_evaluate_accuracy(true_labels, predicted_labels)\n",
    "assert accuracy == 1.0, 'Неверная реализация'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5908cc3-838d-47d4-996c-6093891abaef",
   "metadata": {},
   "source": [
    "## Задание №2\n",
    "\n",
    "Реализуйте алгоритм _иерархической кластеризации на основе связей_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ef074-5dae-4887-9d56-c97e6bd0ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalClustering:\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Инициализация объекта для кластеризации.\n",
    "\n",
    "        :param data: numpy.ndarray, данные для кластеризации\n",
    "        :param labels: numpy.ndarray, истинные метки для оценки accuracy\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.n_samples = len(data)\n",
    "        self.distance_matrix = self._calculate_distance_matrix()\n",
    "        self.clusters = [[i] for i in range(self.n_samples)]  # Каждый объект в своем кластере\n",
    "\n",
    "    def _calculate_distance_matrix(self):\n",
    "        \"\"\"\n",
    "        Вычисление матрицы расстояний между всеми парами точек.\n",
    "\n",
    "        :return: numpy.ndarray, матрица расстояний\n",
    "        \"\"\"\n",
    "        # Вычисляем попарные евклидовы расстояния\n",
    "        distance_matrix = squareform(pdist(self.data, metric='euclidean'))\n",
    "        return distance_matrix\n",
    "\n",
    "    def _compute_cluster_distance(self, cluster1, cluster2, method):\n",
    "        \"\"\"\n",
    "        Вычисление расстояния между двумя кластерами согласно методу связи.\n",
    "\n",
    "        :param cluster1: list of int, индексы точек в первом кластере\n",
    "        :param cluster2: list of int, индексы точек во втором кластере\n",
    "        :param method: str, тип связи ('single', 'complete', 'average')\n",
    "        :return: float, расстояние между кластерами\n",
    "        \"\"\"\n",
    "        distances = self.distance_matrix[np.ix_(cluster1, cluster2)]\n",
    "        if method == 'single':\n",
    "            return ... # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "        elif method == 'complete':\n",
    "            return ... # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "        elif method == 'average':\n",
    "            return np.mean(distances)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown linkage method: {method}\")\n",
    "\n",
    "    def fit(self, method='average'):\n",
    "        \"\"\"\n",
    "        Выполнение кластеризации на основе иерархического алгоритма.\n",
    "\n",
    "        :param method: str, тип связи ('single', 'complete', 'average')\n",
    "        :return: numpy.ndarray, метки кластеров\n",
    "        \"\"\"\n",
    "        clusters = self.clusters.copy()\n",
    "        while len(clusters) > 3:\n",
    "            min_dist = np.inf\n",
    "            pair_to_merge = None\n",
    "            # Найти ближайшую пару кластеров\n",
    "            for i in range(len(clusters)):\n",
    "                for j in range(i + 1, len(clusters)):\n",
    "                    dist = self._compute_cluster_distance(clusters[i], clusters[j], method)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        pair_to_merge = (i, j)\n",
    "            if pair_to_merge is None:\n",
    "                break  # Нет пар для объединения\n",
    "            i, j = pair_to_merge\n",
    "            # Объединить кластеры\n",
    "            new_cluster = clusters[i] + clusters[j]\n",
    "            # Удалить старые кластеры и добавить новый\n",
    "            clusters.pop(j)\n",
    "            clusters.pop(i)\n",
    "            clusters.append(new_cluster)\n",
    "        # Присвоить метки кластерам\n",
    "        cluster_labels = np.zeros(self.n_samples, dtype=int)\n",
    "        for idx, cluster in enumerate(clusters):\n",
    "            for sample_idx in cluster:\n",
    "                cluster_labels[sample_idx] = idx\n",
    "        return cluster_labels\n",
    "\n",
    "    def plot_clusters(self, cluster_labels, method):\n",
    "        \"\"\"\n",
    "        Визуализация кластеров.\n",
    "\n",
    "        :param cluster_labels: numpy.ndarray, метки кластеров\n",
    "        :param method: str, тип связи ('single', 'complete', 'average')\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(self.data[:, 0], self.data[:, 1], c=cluster_labels, cmap='viridis', s=15)\n",
    "        plt.title(f'Кластеры с использованием {method}')\n",
    "        plt.xlabel(\"X1\")\n",
    "        plt.ylabel(\"X2\")\n",
    "        plt.show()\n",
    "\n",
    "# Генерация данных для тестирования\n",
    "np.random.seed(42)\n",
    "test_data, test_labels = make_blobs(n_samples=150, centers=3, cluster_std=1.0, random_state=42)\n",
    "# Тестирование\n",
    "results = {}\n",
    "\n",
    "hc = HierarchicalClustering(test_data, test_labels)\n",
    "for method in ['single', 'complete', 'average']:\n",
    "    print(f\"Тестирование метода связи: {method}\")\n",
    "    # Кластеризация\n",
    "    cluster_labels = hc.fit(method=method)\n",
    "    \n",
    "    # Оценка качества\n",
    "    accuracy, _ = match_and_evaluate_accuracy(test_labels, cluster_labels)\n",
    "    results[method] = accuracy\n",
    "    \n",
    "    # Визуализация кластеров\n",
    "    print(f\"Accuracy для {method} linkage: {accuracy:.2f}\")\n",
    "\n",
    "# Проверка корректности работы\n",
    "for method, acc in results.items():\n",
    "    assert acc > 0.99, f\"Accuracy слишком низкое для {method} linkage: {acc:.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5755c1-56a9-4d5c-92f5-bcdbf57437e1",
   "metadata": {},
   "source": [
    "## Задание №3\n",
    "Примените метод иерархической кластеризации к вашему набору данных, проанализируйте полученные результаты и сделайте соответствующие выводы.\n",
    "Оценка качества реализации является ключевым этапом в машинном обучении. Проведите сравнение результатов вашей реализации с ожидаемыми или эталонными значениями, чтобы убедиться в её корректности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9953e2-cfa9-462d-8ddb-41af3865b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение реализаций\n",
    "comparison_results = []\n",
    "\n",
    "hc = HierarchicalClustering(data, labels)\n",
    "\n",
    "hc.plot_clusters(labels, 'идеальной модели кластеризации')\n",
    "\n",
    "for method in ['single', 'complete', 'average']:\n",
    "    # Scipy реализация\n",
    "    Z = linkage(data, method=method)\n",
    "    cluster_labels_scipy = fcluster(Z, t=3, criterion='maxclust') - 1\n",
    "    scipy_accuracy, _ = match_and_evaluate_accuracy(labels, cluster_labels_scipy)\n",
    "    \n",
    "    # TODO: Допишите код\n",
    "    cluster_labels_our = ...  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "    our_accuracy, _ = match_and_evaluate_accuracy(labels, ... ) # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "    hc.plot_clusters(cluster_labels_our, method)\n",
    "\n",
    "    # Сохранение результатов\n",
    "    comparison_results.append({\n",
    "        \"method\": method,\n",
    "        \"scipy_accuracy\": scipy_accuracy,\n",
    "        \"our_accuracy\": our_accuracy,\n",
    "    })\n",
    "\n",
    "# Вывод результатов\n",
    "for result in comparison_results:\n",
    "    print(f\"Метод: {result['method']}\")\n",
    "    print(f\"  - Точность (Scipy): {result['scipy_accuracy']:.4f}\")\n",
    "    print(f\"  - Точность (Наша реализация): {result['our_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446c17c-74ce-46de-8d31-ec799ac680a7",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "\n",
    "1. Какой метод показал наилучший результат по метрике точности? Почему, на ваш взгляд, этот метод оказался более эффективным?\n",
    "1. Какие ошибки классификации наблюдаются для каждого метода? С чем они могут быть связаны?\n",
    "1. Влияет ли исходное распределение данных (например, наличие шумов или перекрытия классов) на качество кластеризации? Если да, то каким образом?\n",
    "1. Привидите пример, при котором все три вида связей будут давать один и тот же результат кластеризации. Пример должен объяснять суть, а не быть набором значений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452c030-eec5-4d28-9bf1-56243effd35d",
   "metadata": {},
   "source": [
    "**Ответы:**\n",
    "\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b1db8-a6bf-497e-83a3-bfbd91e04ca5",
   "metadata": {},
   "source": [
    "Альтернативой иерархической кластеризации является метод **K-средних**. В его базовом варианте количество кластеров $K$ задаётся заранее как входной параметр, а расстояние между векторами $( \\mathbf{x}, \\mathbf{y})$ понимается в смысле евклидовой метрики:\n",
    "$$\n",
    "\\rho(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\| = \\sqrt{(\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y})}.\n",
    "$$\n",
    "\n",
    "Разбиение обучающего набора $T = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\} \\subset \\mathcal{X}$ на кластеры $\\{C_1, \\dots, C_K\\}$ \n",
    "ищется из условия минимизации среднего квадрата внутрикластерных расстояний между векторами с функцией штрафа:\n",
    "$$\n",
    "E = \\sum_{i=1}^K \\frac{1}{|C_i|} \\sum_{\\mathbf{x}_k, \\mathbf{x}_m \\in C_i} |\\mathbf{x}_k - \\mathbf{x}_m\\|^2 \\to \\min_{\\{C_1, \\dots, C_K\\}}. \\tag{6.2}\n",
    "$$\n",
    "\n",
    "Для пояснения алгоритма, реализующего метод  **K-средних**, величины $E_i$ в (6.2) нагляднее представить в другом виде:\n",
    "$$\n",
    "E_i = \\frac{1}{|C_i|} \\sum_{\\mathbf{x}_k, \\mathbf{x}_m \\in C_i} \\|\\mathbf{x}_k - \\mathbf{x}_m\\|^2 = \\sum_{\\mathbf{x}_k \\in C_i} \\|\\mathbf{x}_k - \\mathbf{m}_i\\|^2, \\tag{6.3}\n",
    "$$\n",
    "где **центроиды**:\n",
    "$$\n",
    "\\mathbf{m}_i = \\frac{1}{|C_i|} \\sum_{\\mathbf{x}_k \\in C_i} \\mathbf{x}_k \\tag{6.4}\n",
    "$$\n",
    "представляют собой средние арифметические векторов кластеров $C_i$.\n",
    "\n",
    "\n",
    "Приведём описание алгоритма, реализующего разбиение заданного обучающего набора $T = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ \n",
    "на кластеры $\\{C_1, \\dots, C_K\\}$ по методу  **K-средних**.\n",
    "\n",
    "### Шаг 1.\n",
    "Инициализация: задать начальный набор центроидов \n",
    "$$\n",
    "\\{\\mathbf{m}_1, \\dots, \\mathbf{m}_K\\} \\subset \\mathcal{X}.\n",
    "$$\n",
    "\n",
    "### Шаг 2.\n",
    "Сформировать кластеры $\\{C_i\\}_{i=1}^K$ из ближайших к центроидам векторов обучающей выборки \n",
    "$T = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ по правилу:\n",
    "$$\n",
    "C_i = \\{\\mathbf{x}_k \\in T \\mid \\|\\mathbf{x}_k - \\mathbf{m}_i\\| = \\min_{i=1, \\dots, K} \\|\\mathbf{x}_k - \\mathbf{m}_i\\|\\}. \\tag{6.7}\n",
    "$$\n",
    "\n",
    "### Шаг 3.\n",
    "Для кластеров $\\{C_i\\}_{i=1}^K$ пересчитать их центроиды (6.4):\n",
    "$$\n",
    "\\mathbf{m}_i = \\frac{1}{|C_i|} \\sum_{\\mathbf{x}_k \\in C_i} \\mathbf{x}_k.\n",
    "$$\n",
    "\n",
    "### Шаг 4.\n",
    "Если все новые кластеры оказались теми же, что и были ранее, то закончить работу алгоритма, иначе перейти на шаг 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0ded2-d8ca-425e-aaf6-92e1c730c7f1",
   "metadata": {},
   "source": [
    "## Задание 4\n",
    "Реализуйте метод кластеризации K-средних. Если вам непонятен шаблон, вы можете реализовать алгоритм удобным для вас способом, но не прибегая к использованию готовых библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b317444f-3768-4f44-a4ab-dcd3948ea034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansCustom:\n",
    "    def __init__(self, n_clusters, max_iter=100, tol=1e-4):\n",
    "        \"\"\"\n",
    "        Инициализация K-means.\n",
    "\n",
    "        :param n_clusters: int, количество кластеров.\n",
    "        :param max_iter: int, максимальное количество итераций.\n",
    "        :param tol: float, критерий остановки по изменению центроидов.\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Обучение модели K-means.\n",
    "\n",
    "        :param X: numpy.ndarray, обучающие данные.\n",
    "        :return: numpy.ndarray, метки кластеров.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # TODO: Добавьте случайную инициализацию центроидов\n",
    "        centroids = ... # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Вычисление расстояний от каждого образца до центроидов\n",
    "            distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "\n",
    "            # TODO: Найдите метки для образцов из Х на основе расстояний\n",
    "            labels = ... # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "            # TODO: Обновите (пересчитайте) центроиды\n",
    "            new_centroids = ... # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "            # Проверка критерия остановки\n",
    "            if np.linalg.norm(new_centroids - centroids) < self.tol:\n",
    "                break\n",
    "\n",
    "            centroids = new_centroids\n",
    "\n",
    "        self.centroids = centroids\n",
    "        self.labels_ = labels\n",
    "        return labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Прогнозирование кластеров для новых данных.\n",
    "\n",
    "        :param X: numpy.ndarray, данные для предсказания.\n",
    "        :return: numpy.ndarray, метки кластеров.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "\n",
    "X = np.array([[1, 1], [1.1, 1.1], [5, 5], [5.1, 5.1], [9, 9], [9.1, 9.1]])\n",
    "true_labels = np.array([0, 0, 1, 1, 2, 2])  # Истинные метки кластеров\n",
    "\n",
    "kmeans = KMeansCustom(n_clusters=3, max_iter=100, tol=1e-4)\n",
    "n_attempts = 10\n",
    "best_accuracy = 0\n",
    "\n",
    "for attempt in range(n_attempts):\n",
    "    kmeans = KMeansCustom(n_clusters=3, max_iter=100, tol=1e-4)\n",
    "    predicted_labels = kmeans.fit(X)\n",
    "    accuracy, _ = match_and_evaluate_accuracy(true_labels, predicted_labels)\n",
    "    best_accuracy = max(best_accuracy, accuracy)\n",
    "\n",
    "\n",
    "assert best_accuracy == 1.0, f\"Тест не пройден: accuracy = {accuracy:.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404eae3e-b2ef-4a19-8da9-9cabd4f49dc2",
   "metadata": {},
   "source": [
    "## Задание 5.1\n",
    "Примените метод K-средних к вашему датасету. Поскольку результат работы алгоритма зависит от начального (случайного) разбиения на кластеры, выполните несколько запусков алгоритма. В качестве итогового результата выберите кластеризацию с минимальной суммой евклидовых расстояний от точек до их центроидов (inertia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d5ca4-a425-42f3-a7c6-e82c6b8acc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Многократный запуск K-means\n",
    "n_runs = 10\n",
    "best_labels = None\n",
    "best_inertia = float('inf')\n",
    "best_centroids = None\n",
    "\n",
    "for run in range(n_runs):\n",
    "    kmeans = KMeansCustom(n_clusters=3)\n",
    "    \n",
    "    predicted_labels = kmeans.fit(data)\n",
    "    # TODO: Добавьте вычисление Евклидовой ошибки (сумма расстояний до ближайших центроидов)\n",
    "    inertia = ... # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "    if inertia < best_inertia:\n",
    "        best_inertia = inertia\n",
    "        best_labels = predicted_labels\n",
    "        best_centroids = kmeans.centroids\n",
    "\n",
    "accuracy, mapping = match_and_evaluate_accuracy(labels, best_labels)\n",
    "print(\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec685e-9ffa-4214-88ef-2234b317a3cf",
   "metadata": {},
   "source": [
    "Визулизируйте результаты кластеризации. Для этого вам необходимо дописать код в функции *visualize_clusters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb4a60-b3b2-4467-b89c-4c5f4858679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(data, true_labels, predicted_labels, mapping, centroids):\n",
    "    \"\"\"\n",
    "    Визуализация кластеров с выделением ошибок.\n",
    "\n",
    "    :param data: numpy.ndarray, данные.\n",
    "    :param true_labels: numpy.ndarray, истинные метки.\n",
    "    :param predicted_labels: numpy.ndarray, предсказанные метки.\n",
    "    :param mapping: dict, сопоставление меток кластеров истинным меткам.\n",
    "    :param centroids: numpy.ndarray, координаты центроидов.\n",
    "    \"\"\"\n",
    "    # Преобразуем предсказанные метки в истинные метки на основе сопоставления\n",
    "    mapped_labels = np.array([mapping[label] for label in predicted_labels])\n",
    "\n",
    "    # Определяем ошибки\n",
    "    errors = mapped_labels != true_labels\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i in np.unique(true_labels):\n",
    "        # Корректно классифицированные точки\n",
    "        plt.scatter(\n",
    "            data[(true_labels == i) & ~errors, 0], \n",
    "            data[(true_labels == i) & ~errors, 1], \n",
    "            label=f'Класс {i}', alpha=1.0\n",
    "        )\n",
    "        # TODO: Добавьте визуализацию для ошибок кластеризации\n",
    "        # Ошибочные точки того же класса\n",
    "        plt.scatter(...)  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "    # центроиды\n",
    "    plt.scatter(\n",
    "        centroids[:, 0], \n",
    "        centroids[:, 1], \n",
    "        c='black', \n",
    "        marker='X', \n",
    "        s=150, \n",
    "        label='Центроиды'\n",
    "    )\n",
    "\n",
    "    plt.title('Результаты кластеризации')\n",
    "    plt.xlabel('Признак 1')\n",
    "    plt.ylabel('Признак 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_clusters(data, labels, best_labels, mapping, best_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79257a1-ea07-45bd-9b0a-2bbdfa4ffa5f",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "\n",
    "1. Почему начальная случайная инициализация центроидов может повлиять на результаты работы метода K-средних? Как можно исправить ситуация, не прибегая к нескольким запускам алгоритма ? \n",
    "1. Что произойдёт, если число кластеров K будет больше или меньше, чем фактическое число групп в данных?\n",
    "1. Как поведёт себя метод K-средних в случае U-образных или вытянутых кластеров ? Для ответа на этот вопрос вам придётся дописать код ниже\n",
    "1. Сравните точность метода K-средних и метода иерархической кластеризации. Кто оказался точнее и почему? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70460e4-ed6e-46bd-a2b6-60e5896782ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "# Генерация данных: вытянутые (U-образные) кластеры\n",
    "u_shaped_data, u_shaped_labels = make_moons(n_samples=500, noise=0.05, random_state=42)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.scatter(u_shaped_data[:, 0], u_shaped_data[:, 1], c=u_shaped_labels, cmap='viridis', s=20)\n",
    "plt.title(\"U-образные кластеры\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "\n",
    "#Ваш код для обучения и визуализции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd694437-f1a4-4e23-9523-1654d9f28f8a",
   "metadata": {},
   "source": [
    "**Ответы:**\n",
    "\n",
    "1. ...\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81932fc-e885-49d9-a28d-d075cfcf432c",
   "metadata": {},
   "source": [
    "## Задание 5.2\n",
    "Сравните вашу реализацию с той, что доступна в пакете sklearn. Допишите код, сравните точность и сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbb47e-b1cf-4491-ada0-4e42758bfab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa85154-1378-4fb3-b8f0-3bad4e8afbc1",
   "metadata": {},
   "source": [
    "**Ваши выводы:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074a302-13b1-4266-bdf8-ed0d2ebc3197",
   "metadata": {},
   "source": [
    "### Модели распределения в виде гауссовых смесей\n",
    "До этого при решении задачи кластеризации мы не привлекали каких-либо моделей для описания законов распределения данных. Вместе с тем на практике могут быть известны как априорные сведения о количестве классов $K$, которые требуется выделить (т.е. известно число классов, представленных в выборке), так и обоснованное предположение о виде внутриклассовых распределений. Эту информацию, конечно, следует использовать.\n",
    "Если предположить, что разделения описываются функциями плотности вероятностей $p(x \\mid C_k) $, то для генеральной совокупности, из которой получена обучающая выборка, плотность вероятности будет иметь вид:\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^K \\alpha_k p(x \\mid C_k), \\tag{6.17}\n",
    "$$\n",
    "где$\\sum_{k=1}^K \\alpha_k = 1.$ Весовой коэффициент $\\alpha_k$ в (6.17) определяет долю класса $C_k$ в общем распределении, которую можно также интерпретировать как априорную вероятность появления этого класса: $\\alpha_k = P(C_k).$\n",
    "\n",
    "Положим, что в заданной обучающей выборке $T = \\{\\mathbf{x}_j\\}_{j=1}^N$\n",
    "представлены образцы из $K$ классов $\\{C_1, \\dots, C_K\\} \\$, каждый из которых имеет многомерное нормальное распределение. Тогда плотность распределения для генеральной совокупности принимает вид гауссовой смеси:\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^K \\alpha_k \\mathcal{N}(x \\mid \\mathbf{m}_k, C_k). \\tag{6.18}\n",
    "$$\n",
    "\n",
    "\n",
    "Далее мы будем исходить из того, что в (6.18) параметры плотностей  $p(x \\mid C_k) = \\mathcal{N}(x \\mid \\mathbf{m}_k, C_k)$\n",
    "(векторы математических ожиданий $\\mathbf{m}_k$ и ковариационные матрицы $C_k$, веса классов $\\alpha_k$ и принадлежность того или иного образа обучающей выборки $T = \\{\\mathbf{x}_j\\}_{j=1}^N$ к тому или иному классу неизвестны.\n",
    "\n",
    "Если бы в выборке $T$ содержались метки принадлежности векторов $\\mathbf{x}_n$ конкретным классам, то найти по ней оценки параметров\n",
    "$$\n",
    "\\Theta = \\{\\alpha_k, \\mathbf{m}_k, C_k\\}_{k=1}^K\n",
    "$$\n",
    "функции (6.18) можно было бы в результате статистического анализа данных. Обозначим множество векторов обучающей выборки, принадлежащих классу $C_k$, как\n",
    "$$\n",
    "\\Omega_k = \\{\\mathbf{x}_n \\in T \\mid y_n = k\\},\n",
    "$$\n",
    "а количество этих векторов как $N_k = |\\Omega_k|$. Для нахождения параметров плотности (6.18) можем использовать следующие хорошо известные из математической статистики точечные оценки \\[6\\]:\n",
    "$$\n",
    "\\alpha_k = \\frac{N_k}{N}, \\quad\n",
    "\\mathbf{m}_k = \\frac{1}{N_k} \\sum_{\\mathbf{x}_n \\in \\Omega_k} \\mathbf{x}_n, \\quad\n",
    "\\hat{C}_k = \\frac{1}{N_k-1} \\sum_{\\mathbf{x}_n \\in \\Omega_k} (\\mathbf{x}_n - \\mathbf{m}_k)(\\mathbf{x}_n - \\mathbf{m}_k)^\\top, \\quad k = 1, \\dots, K. \\tag{6.19}\n",
    "$$\n",
    "\n",
    "Напомним, что при известном (не требующем оценивания) векторе математических ожиданий $\\mathbf{m}_k$ вместо (6.19) для ковариационной матрицы используется оценка:\n",
    "$$\n",
    "\\hat{C}_k = \\frac{1}{N_k} \\sum_{\\mathbf{x}_n \\in \\Omega_k} (\\mathbf{x}_n - \\mathbf{m}_k)(\\mathbf{x}_n - \\mathbf{m}_k)^\\top. \\tag{6.20}\n",
    "$$\n",
    "\n",
    "В рассматриваемой нами задаче метки принадлежности к классам скрыты (неизвестны), и применить напрямую формулы (6.20) мы не можем.\n",
    "\n",
    "\n",
    "Опустив длинные выкладки, приведём ряд формул, которые нам пригодятся:\n",
    "\n",
    "$$\n",
    "\\gamma_{n,k} = P(C_k \\mid \\mathbf{x}_n) = \\frac{\\alpha_k \\mathcal{N}(\\mathbf{x}_n \\mid \\mathbf{m}_k, C_k)}{\\sum_{i=1}^K \\alpha_i \\mathcal{N}(\\mathbf{x}_n \\mid \\mathbf{m}_i, C_i)}, \\tag{6.24}\n",
    "$$\n",
    "— апостериорная вероятность принадлежности вектора $\\mathbf{x}_n$ к классу $C_k$.\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_k = \\frac{1}{\\tilde{N}_k} \\sum_{n=1}^N \\gamma_{n,k} \\mathbf{x}_n, \\tag{6.25}\n",
    "$$\n",
    "где величину\n",
    "$$\n",
    "\\tilde{N}_k = \\sum_{n=1}^N \\gamma_{n,k} = \\sum_{n=1}^N P(C_k \\mid \\mathbf{x}_n) \\tag{6.26}\n",
    "$$\n",
    "\n",
    "Обратим внимание на сходство и различия формулы (6.25), которую можно использовать для формирования оценки $\\hat{\\mathbf{m}}_k$, и выражения для $\\mathbf{m}_k$ в (6.19). Поскольку принадлежность векторов выборки к классам при использовании формулы (6.25) неизвестна, то для формирования оценки $\\hat{\\mathbf{m}}_k$ учитываются уже все векторы из обучающей выборки $T$, но с весами $\\gamma_{n,k} = P(C_k \\mid \\mathbf{x}_n)$, которые характеризуют \"уверенность\" в принадлежности каждого конкретного вектора $\\mathbf{x}_n$ к классу $C_k$.\n",
    "\n",
    "$$\n",
    "\\hat{C}_k = \\frac{1}{\\tilde{N}_k} \\sum_{n=1}^N \\gamma_{n,k} (\\mathbf{x}_n - \\mathbf{m}_k)(\\mathbf{x}_n - \\mathbf{m}_k)^\\top, \\tag{6.27}\n",
    "$$\n",
    "где \n",
    "$\n",
    "\\tilde{N}_k = \\sum_{n=1}^N \\gamma_{n,k}.\n",
    "$\n",
    "$$\n",
    "\\alpha_k = \\frac{1}{N} \\sum_{n=1}^N \\gamma_{n,k} = \\frac{\\tilde{N}_k}{N}. \\tag{6.28}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e2dcb8-d8d6-43e3-a877-bccd89df8599",
   "metadata": {},
   "source": [
    "Как видим, для нахождения параметров $\\{\\alpha_k, \\mathbf{m}_k, C_k\\}_{k=1}^K$ по формулам (6.25)–(6.28) требуется сначала вычислить \"уверенности\" $\\gamma_{n,k}$ по правилу (6.24). Но для использования соотношения (6.24) параметры $\\{\\alpha_k, \\mathbf{m}_k, C_k\\}_{k=1}^K$ уже должны быть известны. Этот замкнутый круг естественным образом наводит на мысль об использовании следующей итерационной процедуры: задав каким-либо образом начальные значения параметров $\\{\\alpha_k^{(0)}, \\mathbf{m}_k^{(0)}, C_k^{(0)}\\}_{k=1}^K$, рассчитать значения \"уверенности\" $\\gamma_{n,k}$ по (6.24) и после этого найти новые значения параметров\n",
    "$$\n",
    "\\{\\alpha_k^{(1)}, \\mathbf{m}_k^{(1)}, C_k^{(1)}\\}_{k=1}^K.\n",
    "$$\n",
    "Когда найденные на очередной итерации параметры\n",
    "$$\n",
    "\\{\\alpha_k^{(i)}, \\mathbf{m}_k^{(i)}, C_k^{(i)}\\}_{k=1}^K\n",
    "$$\n",
    "становятся мало отличимыми от предыдущих значений\n",
    "$$\n",
    "\\{\\alpha_k^{(i-1)}, \\mathbf{m}_k^{(i-1)}, C_k^{(i-1)}\\}_{k=1}^K,\n",
    "$$\n",
    "итерационный процесс завершается. Альтернативным условием завершения работы алгоритма может служить также малое (меньше некоторого порога) изменение функции логарифмического правдоподобия \n",
    "$$\n",
    "L(\\Theta \\mid X) = \\ln \\prod_{n=1}^N p(x_n) = \\sum_{n=1}^N \\ln \\left( \\sum_{k=1}^K \\alpha_k \\mathcal{N}(x_n \\mid \\mathbf{m}_k, C_k) \\right). \\tag{6.21}\n",
    "$$ по сравнению со значением, полученным на предыдущей итерации.\n",
    "\n",
    "### Шаг 1.\n",
    "Инициализация: задать начальные значения параметров\n",
    "$$\n",
    "\\{\\alpha_k^{(0)}, \\mathbf{m}_k^{(0)}, C_k^{(0)}\\}_{k=1}^K,\n",
    "$$\n",
    "и рассчитать соответствующее им значение $L_{\\text{old}}$ функции логарифмического правдоподобия (6.21).\n",
    "\n",
    "\n",
    "\n",
    "### Шаг 2. **Expectation step**.\n",
    "По текущим значениям $\\{\\alpha_k, \\mathbf{m}_k, C_k\\}_{k=1}^K$ вычислить по (6.24) условные вероятности — математические ожидания скрытых переменных:\n",
    "$$\n",
    "\\gamma_{n,k} = P(C_k \\mid \\mathbf{x}_n) = \\mathbb{M}(Z_{n,k}),\n",
    "$$\n",
    "для всех индексов $n = 1, \\dots, N; \\, k = 1, \\dots, K $.\n",
    "\n",
    "### Шаг 3. **Maximization step**.\n",
    "По текущим значениям $\\gamma_{n,k}$ пересчитать по формулам (6.16) — (6.28) значения параметров $\\{\\alpha_k, \\mathbf{m}_k, C_k\\}_{k=1}^K$ и вычислить соответствующее им значение $L_{\\text{new}}$ функции (6.21).\n",
    "\n",
    "### Шаг 4. Проверка условия окончания работы.\n",
    "Если $|L_{\\text{new}} - L_{\\text{old}}| \\leq \\varepsilon$, то завершить работу алгоритма; иначе положить $L_{\\text{old}} = L_{\\text{new}}$ и перейти на шаг 2.\n",
    "\n",
    "---\n",
    "\n",
    "Таким образом, шаг 2 — вычисление условных вероятностей $\\gamma_{n,k} = P(C_k \\mid \\mathbf{x}_n)$ — это нахождение математических ожиданий скрытых переменных. По этим значениям на шаге 3 находятся параметры распределения\n",
    "$$\n",
    "\\Theta = \\{\\alpha_k, \\mathbf{m}_k, C_k\\}_{k=1}^K,\n",
    "$$\n",
    "которые максимизируют функцию логарифмического правдоподобия (6.21)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6621636-7ccf-4907-9d09-79afe4d9c447",
   "metadata": {},
   "source": [
    "## Задание 6.1\n",
    "\n",
    "Реализуйте EM алгоритм. Примените его для кластеризации ваших данных. Визуализируйте результат. Если вам непонятен шаблон, вы можете реализовать алгоритм удобным для вас способом, но не прибегая к использованию готовых библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a5f1a-d323-4e68-b653-d78622dc9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GaussianMixtureEM:\n",
    "    def __init__(self, n_clusters, max_iter=100, tol=1e-4):\n",
    "        \"\"\"\n",
    "        Инициализация алгоритма EM для гауссовой смеси.\n",
    "\n",
    "        :param n_clusters: int, количество кластеров.\n",
    "        :param max_iter: int, максимальное количество итераций.\n",
    "        :param tol: float, критерий остановки по изменению параметров.\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.weights = None  \n",
    "        self.means = None  \n",
    "        self.covariances = None \n",
    "\n",
    "    def initialize_parameters(self, X, initial_labels):\n",
    "        \"\"\"\n",
    "        Инициализация параметров смеси на основе начальных меток.\n",
    "\n",
    "        :param X: numpy.ndarray, данные.\n",
    "        :param initial_labels: numpy.ndarray, начальные метки кластеров.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.array([np.mean(initial_labels == k) for k in range(self.n_clusters)])\n",
    "        self.means = np.array([X[initial_labels == k].mean(axis=0) for k in range(self.n_clusters)])\n",
    "        self.covariances = np.array([\n",
    "            np.cov(X[initial_labels == k].T) + np.eye(n_features) * 1e-6\n",
    "            for k in range(self.n_clusters)\n",
    "        ])\n",
    "\n",
    "    def e_step(self, X):\n",
    "        \"\"\"\n",
    "        E-шаг: вычисление responsibilities (апостериорных вероятностей).\n",
    "        :param X: numpy.ndarray, данные.\n",
    "        :return: numpy.ndarray, матрица responsibilities.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_clusters))\n",
    "        \n",
    "        # TODO: Заполнить responsibilities для каждого кластера k\n",
    "        for k in range(self.n_clusters):\n",
    "            # Используйте multivariate_normal.pdf для вычисления вероятности принадлежности к кластеру\n",
    "            responsibilities[:, k] = ...  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "        \n",
    "        # Нормализовать вероятности, чтобы сумма по строке была равна 1\n",
    "        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "        return responsibilities\n",
    "\n",
    "    def m_step(self, X, responsibilities):\n",
    "        \"\"\"\n",
    "        M-шаг: обновление параметров смеси (весов, средних, ковариаций).\n",
    "\n",
    "        :param X: numpy.ndarray, данные.\n",
    "        :param responsibilities: numpy.ndarray, матрица responsibilities.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        N_k = responsibilities.sum(axis=0) \n",
    "\n",
    "        # TODO: Обновите веса кластеров\n",
    "        self.weights = ...  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "        # TODO: Обновите средние значения кластеров\n",
    "        self.means = ...  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "\n",
    "        # TODO: Обновите ковариационные матрицы\n",
    "        self.covariances = []\n",
    "        for k in range(self.n_clusters):\n",
    "            diff = X - self.means[k]\n",
    "            weighted_diff = responsibilities[:, k][:, np.newaxis] * diff\n",
    "            covariance = ...  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "            covariance += np.eye(n_features) * 1e-6  # Регуляризация\n",
    "            self.covariances.append(covariance)\n",
    "        self.covariances = np.array(self.covariances)\n",
    "\n",
    "    def compute_log_likelihood(self, X, responsibilities):\n",
    "        \"\"\"\n",
    "        Вычисление логарифмической функции правдоподобия.\n",
    "\n",
    "        :param X: numpy.ndarray, данные.\n",
    "        :param responsibilities: numpy.ndarray, матрица responsibilities.\n",
    "        :return: float, значение логарифмической правдоподобности.\n",
    "        \"\"\"\n",
    "        # TODO: Реализуйте вычисление логарифмической функции правдоподобия\n",
    "        log_likelihood = ...  # ПОДСТАВЬТЕ СВОЙ КОД ЗДЕСЬ\n",
    "        return log_likelihood\n",
    "\n",
    "    def fit(self, X, initial_labels):\n",
    "        \"\"\"\n",
    "        Обучение гауссовой смеси методом EM.\n",
    "\n",
    "        :param X: numpy.ndarray, данные.\n",
    "        :param initial_labels: numpy.ndarray, начальные метки кластеров.\n",
    "        \"\"\"\n",
    "        self.initialize_parameters(X, initial_labels)\n",
    "        log_likelihood = 0\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-шаг\n",
    "            responsibilities = self.e_step(X)\n",
    "\n",
    "            # M-шаг\n",
    "            self.m_step(X, responsibilities)\n",
    "\n",
    "            # Вычисление логарифмической правдоподобности\n",
    "            new_log_likelihood = self.compute_log_likelihood(X, responsibilities)\n",
    "            \n",
    "            # Критерий остановки\n",
    "            if abs(new_log_likelihood - log_likelihood) < self.tol:\n",
    "                print(f\"Сходимость достигнута на итерации {iteration}\")\n",
    "                break\n",
    "            log_likelihood = new_log_likelihood\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Прогнозирование меток кластеров на основе обученной гауссовой смеси.\n",
    "\n",
    "        :param X: numpy.ndarray, данные.\n",
    "        :return: numpy.ndarray, метки кластеров.\n",
    "        \"\"\"\n",
    "        responsibilities = self.e_step(X)\n",
    "        return np.argmax(responsibilities, axis=1)  # Метка с максимальной вероятностью\n",
    "\n",
    "\n",
    "# Используем результат K-средних как начальное разбиение\n",
    "em_model = GaussianMixtureEM(n_clusters=3, tol=1e-20, max_iter=100)\n",
    "em_model.fit(data, best_labels)\n",
    "\n",
    "# Байесовская классификация\n",
    "em_labels = em_model.predict(data)\n",
    "\n",
    "# Оценка точности\n",
    "accuracy_em, mapping_em = match_and_evaluate_accuracy(labels, em_labels)\n",
    "print(f\"Accuracy для алгоритма EM: {accuracy_em:.2f}\")\n",
    "\n",
    "# Визуализация результатов\n",
    "plt.scatter(data[:, 0], data[:, 1], c=em_labels, cmap='viridis', s=20)\n",
    "plt.title(\"Результаты EM-кластеризации\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dfdb38-eacf-431b-9aec-79defc43ab7f",
   "metadata": {},
   "source": [
    "## Задание 6.2 \n",
    "Этот метод, аналогично KMeans, реализован в библиотеке sklearn. Оцените качество вашей реализации, сравнив её точность с точностью реализации из sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30454e7-126c-4ba8-84b2-8f9e72f3b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture \n",
    "gmm = GaussianMixture(n_components = 3)\n",
    "gmm_l = GaussianMixture(n_components = 3).fit_predict(data)\n",
    "match_and_evaluate_accuracy(labels, gmm_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57394dc2-2417-4fd1-920c-76a9d39e90bf",
   "metadata": {},
   "source": [
    "**Ваши выводы**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60870d95-89cf-4dda-b1bc-e5fe387e6714",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "\n",
    "1. Почему для начальной инициализации параметров используются результаты K-средних? Какие проблемы могут возникнуть при случайной инициализации?\n",
    "1. В чём отличие EM-алгоритма от K-средних в плане подхода к кластеризации? Какие преимущества предоставляет EM-алгоритм? Что у них общего?\n",
    "1. Какие предположения делает гауссова смесь о данных? В каких случаях использование EM-алгоритма будет неэффективным?\n",
    "1. Как поведёт себя алгоритм EM в случае U-образных кластеров ? Для ответа на этот вопрос вам придётся дописать код ниже\n",
    "1. Сравните точность метода K-средних и EM-алгоритма. Кто оказался точнее? Почему ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eccebd-b86d-419a-9634-7280f3751a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "# Генерация данных: вытянутые (U-образные) кластеры\n",
    "u_shaped_data, u_shaped_labels = make_moons(n_samples=500, noise=0.05, random_state=42)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.scatter(u_shaped_data[:, 0], u_shaped_data[:, 1], c=u_shaped_labels, cmap='viridis', s=20)\n",
    "plt.title(\"U-образные кластеры\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "\n",
    "#Ваш код для обучения и визуализции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffea13-964d-48c6-ba2f-84bf59e23da3",
   "metadata": {},
   "source": [
    "**Ответы:**\n",
    "\n",
    "1. ...\n",
    "1. ...\n",
    "1. ...\n",
    "1. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
